# Test Analytics Features

{:toc}

## Find flaky tests

See which tests are most disruptive to continuous deployments via reliability scores and trends on both your entire test suite and individual tests.

<strong>Reliablity</strong> is calculated as follows:

- Test suite reliability = passed_runs / (passed_runs + failed_runs) * 100
- Individual test reliability = passed_test_executions / (passed_test_executions + failed_test_executions) * 100

(Other test execution results, such as `unknown` and `skipped`, are ignored in the test reliability calculation.)

`‚ùóImportant:` In Test Analytics, a run is marked as red/failed as soon as a test execution fails (regardless of whether it passes on a retry) to help surface unreliable tests. You can have a situation where a build eventually passes on retry in a Pipeline, and the related run is marked as red/failed in Test Analytics.

<em><strong>Coming soon:</strong> even more granularity into run states (such as in progress, finished, and information about retries), with web hooks so that you can pull that information into your preferred platform.</em>


## Spot sneaky slow-downs

Find performance degradations, even in parallelized builds. Track down what's slowing down your test runs and individual test executions by viewing trends over time, and then dive deep into individual test execution span timelines to identify patterns.

<%= image "test-trend.png", width: 1175, height: 474, alt: "Screenshot of test trend page showing change in duration across test runs and a recent failed test execution" %>

<%= image "span-timeline.png", width: 1125, height: 451, alt: "Screenshot of span timeline with user-defined annotation" %>


